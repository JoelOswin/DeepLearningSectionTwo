{"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1Ig3SrBXmE9BCTIDKtKgq9y5Eo7d5MvEd","authorship_tag":"ABX9TyMT4FRlU11J7iPrat8goLlC"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"ea53236197f6452d8ab87638e73e6455":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c9d3b0df6b11429da670df818c07bb2b","IPY_MODEL_addac0401d654ee5b50e2c08bcc7b100","IPY_MODEL_cba16768428f41e688292f2b4d97a814"],"layout":"IPY_MODEL_4a6ffd3a7a8141c4baa85e5fc7f20ce3"}},"c9d3b0df6b11429da670df818c07bb2b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_81c75e59a136429cb0e716a7c21813e0","placeholder":"​","style":"IPY_MODEL_2104b0d362974b5fad46fd5e8572bbca","value":"Map:   0%"}},"addac0401d654ee5b50e2c08bcc7b100":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_65b47a66674e4510a429203c633079a7","max":28292,"min":0,"orientation":"horizontal","style":"IPY_MODEL_80d9208d21294fb68b69d74d2265fe95","value":0}},"cba16768428f41e688292f2b4d97a814":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d4fd21b2a5a848a8b2f63eb2a3d5e970","placeholder":"​","style":"IPY_MODEL_b0dc93d4bacf49be96fa655e36430768","value":" 0/28292 [00:00&lt;?, ? examples/s]"}},"4a6ffd3a7a8141c4baa85e5fc7f20ce3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81c75e59a136429cb0e716a7c21813e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2104b0d362974b5fad46fd5e8572bbca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"65b47a66674e4510a429203c633079a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80d9208d21294fb68b69d74d2265fe95":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d4fd21b2a5a848a8b2f63eb2a3d5e970":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0dc93d4bacf49be96fa655e36430768":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9341282,"sourceType":"datasetVersion","datasetId":5661138}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![image.png](data:image/png;base64,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)","metadata":{"id":"JhG-Jmq7wBBJ"}},{"cell_type":"markdown","source":"**INSTALLING REQUIRED PACKAGES**","metadata":{"id":"OJbwtmEIwF3p"}},{"cell_type":"code","source":"!pip install -U datasets","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"maUSY3CVhzNx","executionInfo":{"status":"ok","timestamp":1725735930001,"user_tz":-330,"elapsed":4473,"user":{"displayName":"Joel Oswin","userId":"15002773100147134904"}},"outputId":"ac6021e8-46d1-46bf-c097-f2b4634908cc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n!pip install transformers==4.19.2\n!pip install rouge_score\n!pip install rouge","metadata":{"id":"7ni3TDvKiCjs","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**DATA PREPROCESSING**\n\n*   Data Processing is done to make the data ready to be studied by the pre-trained model\n*   Firstly, the csv file obtained in section 1 is read using pandas module\n* The read_csv method of pandas(pd) module is used to read the csv file\n* The df.head() returns the 1st five entries of the data\n\n","metadata":{"id":"NiDccS_dwPWl"}},{"cell_type":"code","source":"from datasets import load_metric\nimport pandas as pd\ndf = pd.read_csv(\"/kaggle/input/wikihow-csv/wikiHow.csv\",skip_blank_lines=True)\ndf.head()\nimport os\nos.environ[\"WANDB_DISABLED\"]=\"true\"","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"MKdSHMMWiIoT","executionInfo":{"status":"ok","timestamp":1725735939223,"user_tz":-330,"elapsed":1353,"user":{"displayName":"Joel Oswin","userId":"15002773100147134904"}},"outputId":"f623c46c-958b-41a3-9e5e-e3f91dbfda1f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"id":"HpJcoL2KiRHb","executionInfo":{"status":"ok","timestamp":1725735939223,"user_tz":-330,"elapsed":6,"user":{"displayName":"Joel Oswin","userId":"15002773100147134904"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e5cae470-0fe8-4fcb-9b18-289e20328ccd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Empty lines and the duplicates must be removed from the data. Duplicates are generated because the url is produced randomly in section one so there’s a chance for the same url to randomly produced. Thankfully these empty and duplicate lines can removed with the help of dropna(inplace=True) and df.drop_duplicates(inplace = True) respectively.\n      \nThe inplace=True argument changes the original dataframe. It is by default False. If False it creates a altered copy of the original dataframe.","metadata":{"id":"7UNPUlzWxBpy"}},{"cell_type":"code","source":"df.dropna(inplace=True)\nprint(df.shape)","metadata":{"id":"z6nRuc3PkQph","executionInfo":{"status":"ok","timestamp":1725735939224,"user_tz":-330,"elapsed":7,"user":{"displayName":"Joel Oswin","userId":"15002773100147134904"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"15bb9d10-86da-46a4-84f2-77e85a75ea8a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop_duplicates(inplace=True)\nprint(df.shape)","metadata":{"id":"_w_37pyxkaVw","executionInfo":{"status":"ok","timestamp":1725735939224,"user_tz":-330,"elapsed":5,"user":{"displayName":"Joel Oswin","userId":"15002773100147134904"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"21ea1f03-3d7a-4947-9bd4-d88c21f70dc8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**DEALING WITH OUTLIERS**\n\nOutliers happens when the entries are either too low or too high. In order to differentiate the outliers from the rest of the data, we need to plot the data using pyplot of matplotlib module. Before that, we must create another column called length which specifies the length of the paragraph.","metadata":{"id":"VUE_sg3LxeGZ"}},{"cell_type":"markdown","source":"The lambda function splits the paragraph by the \" \" character which is nothing but the spaces between words and the len(x.split(“ “)) returns the number of words in the paragraph.\n\nThe paragraph are passed as arguments to the lambda function using map method Thus the fourth column is created.","metadata":{"id":"mjtvwdQ_xzon"}},{"cell_type":"code","source":"df['length'] = df.paragraph.map(lambda x: len(x.split(\" \")))","metadata":{"id":"PQK0rC_hmnoG","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**PLOTTING THE DATA**\n\nWe begin by assigning the df.length to a variable named numOfWords to ensure that no changes will be made to the original variable, df. After importing pyplot of Mathplotlib as plt, we define the figure size to be (5,3) (Check the circled area below) which implies the width of the figure must be 5 inches wide and it’s length must be 3 inches.\n\nnumOfWords is an one dimensional ndarray which is converted into to a numpy array so that plt.hist can read the data\n\nThe bins argument represents the bin edges, that is the starting point and the ending point for the bins along the x-axis. For this histogram figure, the edges are [0,50,100,200,300,500,1000]. Finally plt.show() prints the figure.","metadata":{"id":"rEaDviw9yTpN"}},{"cell_type":"code","source":"numOfWords = df.length\nfrom matplotlib import pyplot as plt\n\nfig=plt.figure(figsize=(5,3))\nplt.hist(numOfWords.to_numpy(), bins=[0,50,100,200,300,500,1000])\n\nplt.title(\"Word Count Distribution\")\n\nplt.show()","metadata":{"id":"ZYOLNEPzkumY","executionInfo":{"status":"ok","timestamp":1725735939699,"user_tz":-330,"elapsed":479,"user":{"displayName":"Joel Oswin","userId":"15002773100147134904"}},"colab":{"base_uri":"https://localhost:8080/","height":314},"outputId":"b067a7b0-1d24-41d5-d141-2d0f3ef51d37","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**IDENTIFYING AND REMOVING THE OUTLIERS**\n\nFrom the above figure, we can conclude that most of the paragraphs have words more than thousand and very few paragraphs have words more than 200. Therefore, **these paragraphs with more than 200 words are the outliers**. Hence, we must remove them to have the data ready for further processing. The Below block of code removes the outliers.","metadata":{"id":"6_XViY4tzRM4"}},{"cell_type":"code","source":"tempDf = df[df.length <= 200]\ntempDf.shape","metadata":{"id":"w6Muk4ZTnE5w","executionInfo":{"status":"ok","timestamp":1725735939699,"user_tz":-330,"elapsed":5,"user":{"displayName":"Joel Oswin","userId":"15002773100147134904"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b5a56be7-e9bf-450a-b772-d80a81e76367","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From this, we infer that there was 804 outliers in the dataframe.\n\n**THIS CONCLUDES DATA PREPROCESSING**","metadata":{"id":"qq516EmhzlWr"}},{"cell_type":"markdown","source":"**FINE-TUNING WITH LED**\n\nFine-tuning is the process of training a pre-trained model using the preprocessed dataframe to give meaningful results in our case, relevant titles according to the paragraph\n\nIn Our Exercise, we fine-tune the language model called Longformer Encoder Decoder(LED)\n\n**TOKENIZATION**\n\nTokenization is the process of breaking down the data into smaller segments(tokens) which can be understood by machines. Tokenization enables machines to understand Human Language.","metadata":{"id":"2eui536izpwb"}},{"cell_type":"code","source":"from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"allenai/led-base-16384\")","metadata":{"id":"NDlrCBhWniEC","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We divide the data in the dataframe into batches to make the training process manageable and efficient. This also prevents overfitting since the process can be interrupted. Overfitting happens when the model memorizes the data instead of learning valuable patterns from the data. Using smaller batches allows the model to diverse into complicated examples and ultimately prevents overfitting. However, the batch size must not be too small. Very Small batches results in the model being inconsistent. Finding the optimal batch size is essential to train a Model. Now we define the function required to process data to model inputs","metadata":{"id":"G7L0BicZ1IQC"}},{"cell_type":"code","source":"max_input_length = 1024\nmax_output_length = 64\nbatch_size = 16\n\ndef process_data_to_model_inputs(batch):\n  #tokenize the inputs and outputs\n  inputs = tokenizer(\n      batch[\"paragraph\"],\n      padding=\"max_length\",\n      truncation=True,\n      max_length=max_input_length,\n  )\n\n  outputs = tokenizer(\n      batch[\"heading\"],\n      padding=\"max_length\",\n      truncation=True,\n      max_length=max_output_length,\n  )\n\n  batch[\"input_ids\"]=inputs.input_ids\n  batch[\"attention_mask\"]=inputs.attention_mask\n\n  #Create 0 global_attention_mask lists\n  batch[\"global_attention_mask\"] = len(batch[\"input_ids\"]) * [\n      [0 for _ in range(len(batch[\"input_ids\"][0]))]\n  ]\n\n  #since above lists are refrences, the following line changes the 0 index of all samples\n  batch[\"global_attention_mask\"][0][0] = 1\n  batch[\"labels\"] = outputs.input_ids\n\n  #We have to make sure that the PAD token is ignored\n  batch[\"labels\"] = [\n      [-100 if token == tokenizer.pad_token_id else token for token in labels]\n      for labels in batch[\"labels\"]\n  ]\n\n  return batch","metadata":{"id":"mSLRlBxOoWpn","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n* Inputs and Outputs are tokenized using the tokenizer created in the previous page\n      \n* The Inputs are the paragraphs and these paragraphs are divided into batches\n      \n* padding is done so that every input in the batches have the same length (1024)\n      \n* Truncation is set to be true in order to match the max input length(Here, 1024)\n      \n* max length is set to be max input length(1024)\n      \n* The Same procedure is followed for output tokens except the outputs are the headings\n\n* Attention Mask controls which input sequence the model should pay attention to\n      \n* The Attention Mask is first set to 0 then the attention mask of every input is set to 1 ensuring equal importance to every input in the batch\n      \n* Tokens must always be positive(>0). The Tokens which are negative are ignored by the model\n      \n* Therefore we set the PAD tokens to be -100 so that they are ignored\n\n","metadata":{"id":"ZVliNNf70DC_"}},{"cell_type":"code","source":"import numpy as np\ntrain,validate,test = np.split(tempDf.sample(frac=1, random_state=42),[int(.6*len(df)), int(.7*len(df))])\nprint(train.shape)\nprint(validate.shape)\nprint(test.shape)","metadata":{"id":"FxEwIoSOA6Sx","executionInfo":{"status":"ok","timestamp":1725735941204,"user_tz":-330,"elapsed":8,"user":{"displayName":"Joel Oswin","userId":"15002773100147134904"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c0b9e4ff-1fac-4332-ff4d-68f18d2bacb1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The DataFrame, tempDf is split into train, validate and test cases using split method of numpy\n      \n* sample method is used to randomize the dataframe\n      \n* frac argument of sample method represents the percentage of data to be taken from the total data for example if frac=0.5, then only 50% of the entire data will be taken. Here frac is equal to 1 so the entire dataframe will be taken.\n      \n* random_state argument randomizes the data.\n","metadata":{"id":"BtveX2tL1rP2"}},{"cell_type":"markdown","source":"Validate set is reduced to make the training process faster but in practice, it should not be reduced","metadata":{"id":"qc__Y64E1ez_"}},{"cell_type":"code","source":"validate = validate[:20]","metadata":{"id":"jOFCeaoxB9iX","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validate.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_oosZ0l3DMO_","executionInfo":{"status":"ok","timestamp":1725735941204,"user_tz":-330,"elapsed":7,"user":{"displayName":"Joel Oswin","userId":"15002773100147134904"}},"outputId":"913f48ce-ee84-42be-fc94-f0fcbcd0565b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**CREATING DATASETS**\n\n* We import Dataset from datasets module\n      \n* The Datasets are created using the from_pandas method of Dataset","metadata":{"id":"oPuL9N9Z2Bo7"}},{"cell_type":"code","source":"from datasets import Dataset\ntrain_dataset = Dataset.from_pandas(train)\nval_dataset = Dataset.from_pandas(validate)","metadata":{"id":"ovMlrXhQDNd5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"__TRAINING DATASETS__\n\n* train_dataset obtained in previous page is passed as an argument to process_data_model_inputs defined earlier using the map method\n      \n* All the unnecessary columns are removed","metadata":{}},{"cell_type":"code","source":"train_dataset = train_dataset.map(\n    process_data_to_model_inputs,\n    batched=True,\n    batch_size = batch_size,\n    remove_columns=[\"title\",\"heading\",\"paragraph\",\"length\",\"__index_level_0__\"],\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":388,"referenced_widgets":["ea53236197f6452d8ab87638e73e6455","c9d3b0df6b11429da670df818c07bb2b","addac0401d654ee5b50e2c08bcc7b100","cba16768428f41e688292f2b4d97a814","4a6ffd3a7a8141c4baa85e5fc7f20ce3","81c75e59a136429cb0e716a7c21813e0","2104b0d362974b5fad46fd5e8572bbca","65b47a66674e4510a429203c633079a7","80d9208d21294fb68b69d74d2265fe95","d4fd21b2a5a848a8b2f63eb2a3d5e970","b0dc93d4bacf49be96fa655e36430768"]},"id":"_ZFuhH5DHkTd","executionInfo":{"status":"error","timestamp":1725736463811,"user_tz":-330,"elapsed":1042,"user":{"displayName":"Joel Oswin","userId":"15002773100147134904"}},"outputId":"7b0dc318-a943-417a-faed-0cb0a0b2ac45","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The same procedure is followed for val_dataset","metadata":{"id":"dKFRoE1tvw-E"}},{"cell_type":"code","source":"val_dataset = val_dataset.map(\n    process_data_to_model_inputs,\n    batched=True,\n    batch_size = batch_size,\n    remove_columns=[\"title\",\"heading\",\"paragraph\",\"length\",\"__index_level_0__\"],\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Setting the format of train and val dataset","metadata":{}},{"cell_type":"code","source":"train_dataset.set_format(\n    type = \"torch\",\n    columns = [\"input_ids\",\"attention_mask\",\"global_attention_mask\",\"labels\"],\n)\n\nval_dataset.set_format(\n    type = \"torch\",\n    columns = [\"input_ids\",\"attention_mask\",\"global_attention_mask\",\"labels\"],\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating the necessary configurations","metadata":{}},{"cell_type":"code","source":"\nfrom transformers import AutoModelForSeq2SeqLM\nfrom datasets import load_metric\nled = AutoModelForSeq2SeqLM.from_pretrained(\"allenai/led-base-16384\", gradient_checkpointing=True, use_cache=False)\nled.config.num_beams = 2\nled.config.max_length = 64\nled.config.min_length = 2\nled.config.length_penalty = 2.0\nled.config.early_stopping = True \nled.config.no_repeat_ngram_size = 3 \nrouge = load_metric(\"rouge\")\n\ndef compute_metrics(pred):\n    labels_ids = pred.label_ids\n    pred_ids = pred.predictions\n    \n    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True) \n    labels_ids[labels_ids == -100]= tokenizer.pad_token_id\n    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n    \n    rouge_output = rouge.compute(\n        predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"]\n    )[\"rouge2\"].mid\n    \n    return {\n        \"rouge2_precision\": round(rouge_output.precision, 4), \n        \"rouge2_recall\": round(rouge_output.recall, 4),\n        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n    }\n\nfrom transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments \nimport transformers\ntransformers.logging.set_verbosity_info()\n\ntraining_args = Seq2SeqTrainingArguments(\n    predict_with_generate=True,\n    evaluation_strategy=\"steps\",\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    output_dir=\"./\",\n    logging_steps=5,\n    eval_steps=10,\n    save_steps=10,\n    save_total_limit=2,\n    gradient_accumulation_steps=4,\n    num_train_epochs=10\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\nmodel=led,\ntokenizer=tokenizer,\nargs=training_args,\ncompute_metrics=compute_metrics,\ntrain_dataset=train_dataset,\neval_dataset=val_dataset,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training the model","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Testing The Model","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nsample_paragraph = \"The reason why I loved the top-down culture at Apple is that important decisions are taken faster. Having an expert giving you green light or not keeps the momentum. How many times in a bottom-up culture do we spend weeks and weeks, sometimes even months, trying to get alignment with +10 people, because every single person needs to agree with the point of view? It is exhausting. So again, my experience is that having that one leader to look up to to help guide decisions is time-saving, it helps us focus on the design craft, instead of project management\"\ndf = pd.DataFrame(data,columns=[\"paragraph\"])\ndf[\"paragraph\"][0]\nfrom datasets import Dataset\ndf_test = Dataset.from_pandas(df)\ndf_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_metric\nimport torch\n\nfrom datasets import load_dataset,load_metric\nfrom transformers import LEDTokenizer,LEDForConditionalGeneration\n\n#load tokenizer\ntokenizer = LEDTokenizer.from_pretrained(\"/kaggle/working/checkpoint-110\")\nmodel = LEDForConditionalGeneration.from_pretrained(\"/kaggle/working/checkpoint-110\").to(\"cuda\").half()\n\ndef generate_answer(batch):\n    inputs_dict = tokenizer(batch[\"paragraph\"], padding=\"max_length\", max_length=512, return_tensors=\"pt\", truncation=True)\n    input_ids = input_dict.input_ids.to(\"cuda\")\n    attention_mask = inputs_dict.attention_mask.to(\"cuda\")\n    global_attention_mask = torch.zeros_like(attention_mask)\n    \n    predicted_abstract_ids = model.generate(input_ids, attention_mask=attention_mask,global_attention_mask=global_attention_mask)\n    batch[\"generated_heading\"] = tokenizer.batch_decode(predicted_abstract_ids, skip_special_tokens=True)\n    return batch\n\nresult = df_test.map(generate_answer, batched=True, batch_size=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result[\"generated_heading\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}